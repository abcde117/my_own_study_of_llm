{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrXIQAJrIF9M"
      },
      "source": [
        "#pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYeIHvPnZqOd",
        "outputId": "58cce3c3-2590-47fb-c30f-ef44d7ad9fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (1.26.16)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (16.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchdata) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "--2023-07-31 12:01:47--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt’\n",
            "\n",
            "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-31 12:01:47 (22.5 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata\n",
        "!pip install 'portalocker>=2.0.0'\n",
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu3BnzZ2IFOR",
        "outputId": "9ac0e7a4-4516-4630-f5b7-dcda9799b009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9rs6XTuyrYk"
      },
      "source": [
        "#func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaTs1iBMZs6s"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from random import *\n",
        "\n",
        "import sentencepiece as spm\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "from typing import List\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import unicodedata\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bbCpfN3alM8"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XW8IwQbZxx6"
      },
      "source": [
        "#tokenizer&model&llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4GLYi2dZwfE"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self, model_path: str):\n",
        "        # reload tokenizer\n",
        "        assert os.path.isfile(model_path), model_path\n",
        "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
        "        self.n_words: int = self.sp_model.vocab_size()\n",
        "        self.bos_id: int = self.sp_model.bos_id()\n",
        "        self.eos_id: int = self.sp_model.eos_id()\n",
        "        self.pad_id: int = self.sp_model.pad_id()\n",
        "        #logger.info(f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\")\n",
        "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
        "\n",
        "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
        "        assert type(s) is str\n",
        "        t = self.sp_model.encode(s)\n",
        "        if bos:\n",
        "            t = [self.bos_id] + t\n",
        "        if eos:\n",
        "            t = t + [self.eos_id]\n",
        "        return t\n",
        "    def decode(self, t: List[int]) -> str:\n",
        "        return self.sp_model.decode(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH9nRqNVZ2oU"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS6q0ZRwZ4hJ"
      },
      "outputs": [],
      "source": [
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
        "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis\n",
        "\n",
        "\n",
        "def apply_rotary_emb(\n",
        "    xq: torch.Tensor,\n",
        "    xk: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    #print(freqs_cis.size())\n",
        "    #print(xq_.size())\n",
        "    #print(xq_.shape[1:])\n",
        "    freqs_cis = freqs_cis.view(xq_.shape[1:])\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9w0SWIGZ7wr"
      },
      "outputs": [],
      "source": [
        "class ModelArgs:\n",
        "    dim: int = 512\n",
        "    n_layers: int = 3\n",
        "    n_heads: int = 8\n",
        "    vocab_size: int = 3000 # defined later by tokenizer\n",
        "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
        "    norm_eps: float = 1e-5\n",
        "\n",
        "    max_batch_size: int = 32\n",
        "    max_seq_len: int = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Hrip6Tqkt0",
        "outputId": "e0c4c916-ecf0-42e4-e858-45c052ca6673"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "512//8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S5WHXe2qj98"
      },
      "outputs": [],
      "source": [
        "w=nn.Linear(512,512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FRuexHZZ9nL"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads=args.n_heads\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.wq=nn.Linear(args.dim,self.head_dim*args.n_heads)\n",
        "        self.wk=nn.Linear(args.dim,self.head_dim*args.n_heads)\n",
        "        self.wv=nn.Linear(args.dim,self.head_dim*args.n_heads)\n",
        "        self.wo=nn.Linear(self.head_dim*args.n_heads,args.dim)\n",
        "\n",
        "        self.cache_k = torch.zeros(\n",
        "            (args.max_batch_size, args.max_seq_len, args.n_heads, self.head_dim)\n",
        "        ).cuda()\n",
        "        self.cache_v = torch.zeros(\n",
        "            (args.max_batch_size, args.max_seq_len, args.n_heads, self.head_dim)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
        "        bsz, seqlen,_ = x.shape\n",
        "        xq=self.wq(x).view(bsz, -1, self.n_heads, self.head_dim)\n",
        "        xk=self.wk(x).view(bsz, -1, self.n_heads, self.head_dim)\n",
        "        xv=self.wv(x).view(bsz, -1, self.n_heads, self.head_dim)\n",
        "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
        "        self.cache_k[:bsz, start_pos : start_pos + seqlen].data = xk\n",
        "        self.cache_v[:bsz, start_pos : start_pos + seqlen].data = xv\n",
        "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
        "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
        "        xq = xq.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        print(xq.is_cuda)\n",
        "        print(keys.is_cuda)\n",
        "        print(values.is_cuda)\n",
        "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
        "        output = output.transpose(\n",
        "            1, 2\n",
        "        ).contiguous().view(bsz, seqlen, -1)\n",
        "\n",
        "        return self.wo(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY-edpY0Z_fP"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        hidden_dim: int,\n",
        "        multiple_of: int,):\n",
        "\n",
        "        super().__init__()\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "        self.w1=nn.Linear(dim,hidden_dim)\n",
        "        self.w2=nn.Linear(hidden_dim,dim)\n",
        "        self.w3=nn.Linear(dim,hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flmmwsx2aBCN"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.attention = Attention(args)\n",
        "        self.feed_forward = FeedForward(dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of)\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
        "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
        "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1gXHwgBaC3O"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, params: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.n_layers = params.n_layers\n",
        "        self.tok_embeddings=nn.Embedding( params.vocab_size, params.dim)\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for layer_id in range(params.n_layers):\n",
        "            self.layers.append(TransformerBlock(layer_id, params))\n",
        "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "        self.output=nn.Linear(params.dim,params.vocab_size)\n",
        "        self.freqs_cis = precompute_freqs_cis(self.params.dim, self.params.max_seq_len * 2)\n",
        "\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
        "        _bsz, seqlen = tokens.shape\n",
        "        h = self.tok_embeddings(tokens)\n",
        "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
        "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
        "\n",
        "        mask = None\n",
        "        if seqlen > 1:\n",
        "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
        "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, start_pos, freqs_cis, mask)\n",
        "        h = self.norm(h)\n",
        "        output = self.output(h)\n",
        "        return output.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2T7MdVngFUB"
      },
      "outputs": [],
      "source": [
        "class LM:\n",
        "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        max_gen_len: int,\n",
        "        temperature: float = 0.8,\n",
        "        top_p: float = 0.95,\n",
        "    ) -> List[str]:\n",
        "        bsz = len(prompts)\n",
        "        params = self.model.params\n",
        "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
        "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
        "        min_prompt_size = min([len(t) for t in prompt_tokens])\n",
        "        max_prompt_size = max([len(t) for t in prompt_tokens])\n",
        "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
        "        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n",
        "        for k, t in enumerate(prompt_tokens):\n",
        "            tokens[k, : len(t)] = torch.tensor(t).long()\n",
        "        input_text_mask = tokens != self.tokenizer.pad_id\n",
        "\n",
        "        start_pos = min_prompt_size\n",
        "        prev_pos = 0\n",
        "        for cur_pos in range(start_pos, total_len):\n",
        "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)[:,-1,:]\n",
        "            if temperature > 0:\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                next_token = sample_top_p(probs, top_p)\n",
        "            else:\n",
        "                next_token = torch.argmax(logits, dim=-1)\n",
        "            next_token = next_token.reshape(-1)\n",
        "            # only replace token if prompt has already been generated\n",
        "            next_token = torch.where(\n",
        "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
        "            )\n",
        "            tokens[:, cur_pos] = next_token\n",
        "            prev_pos = cur_pos\n",
        "\n",
        "        decoded = []\n",
        "        for i, t in enumerate(tokens.tolist()):\n",
        "            # cut to max gen len\n",
        "            t = t[: len(prompt_tokens[i]) + max_gen_len]\n",
        "            # cut to eos tok if any\n",
        "            try:\n",
        "                t = t[: t.index(self.tokenizer.eos_id)]\n",
        "            except ValueError:\n",
        "                pass\n",
        "            decoded.append(self.tokenizer.decode(t))\n",
        "        return decoded\n",
        "\n",
        "\n",
        "def sample_top_p(probs, p):\n",
        "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    mask = probs_sum - probs_sort > p\n",
        "    probs_sort[mask] = 0.0\n",
        "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "    next_token = torch.gather(probs_idx, -1, next_token)\n",
        "    return next_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2sKd9-8aDWZ"
      },
      "source": [
        "#token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aufy9rp-M3E0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDOUztsQaEwB"
      },
      "outputs": [],
      "source": [
        "spm.SentencePieceTrainer.Train(\n",
        "    input='botchan.txt',\n",
        "    model_prefix='m',\n",
        "    vocab_size=3000,\n",
        "    pad_id=3)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "tokenizer=Tokenizer('m.model')\n",
        "train_iter = WikiText2(split='train')\n",
        "sentence=[]\n",
        "for item in train_iter :\n",
        "  if item.strip() == '':pass\n",
        "\n",
        "  elif len(item.strip()) <=90:pass\n",
        "  elif len(item.strip()) >=800:pass\n",
        "  else :sentence.append((item.strip()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b5eAns4aNJi"
      },
      "outputs": [],
      "source": [
        "token=[tokenizer.encode(x, bos=True, eos=True) for x in sentence]\n",
        "max_batch_size: int = 32\n",
        "max_seq_len: int = 2048\n",
        "min_prompt_size = min([len(t) for t in token])\n",
        "max_prompt_size = max([len(t) for t in token])\n",
        "total_len = min(max_seq_len, 30 + max_prompt_size)\n",
        "\n",
        "bsz=len(token)\n",
        "\n",
        "tokens=torch.full((bsz, total_len),tokenizer.pad_id)\n",
        "for k, t in enumerate(token):\n",
        "            tokens[k, : len(t)] = torch.tensor(t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exPUg7ALaYkS"
      },
      "outputs": [],
      "source": [
        "labels=torch.full((bsz, total_len),tokenizer.pad_id)\n",
        "rand = torch.rand(tokens.shape)\n",
        "mask_arr = rand < 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jk8UxZ4ae71"
      },
      "outputs": [],
      "source": [
        "for k,t in enumerate(labels):\n",
        "     if random()>=0.1:\n",
        "            labels[k][torch.where(mask_arr[k] == 1)] =tokens[k][torch.where(mask_arr[k] == 1)]\n",
        "     else:\n",
        "      labels[k][torch.where(mask_arr[k] == 1)]  =torch.tensor([randint(0,2999) for i in  range(len(labels[k][torch.where(mask_arr[k] == 1)])) ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojufj2fJanBC"
      },
      "outputs": [],
      "source": [
        "lines = open('formatted_movie_lines.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrpiVguYaq3I"
      },
      "outputs": [],
      "source": [
        "con1=[ pairs[i][0] for i in range(len(pairs))]\n",
        "con2=[ pairs[i][1] for i in range(len(pairs))]\n",
        "sentence1=[]\n",
        "sentence2=[]\n",
        "for i in range(len(con1)) :\n",
        "  if len(con1[i]) <=100 and len(con2[i])  <=100:\n",
        "    sentence1.append(con1[i])\n",
        "    sentence2.append(con2[i])\n",
        "  else : pass\n",
        "ts1=[tokenizer.encode(x, bos=True, eos=True) for x in sentence1]\n",
        "ts2=[tokenizer.encode(x, bos=True, eos=True) for x in sentence2]\n",
        "bsz=len(sentence1)\n",
        "tokens1=torch.full((bsz, 100),tokenizer.pad_id)\n",
        "for k, t in enumerate(ts1):\n",
        "            tokens1[k, : len(t)] = torch.tensor(t)\n",
        "\n",
        "tokens2=torch.full((bsz, 100),tokenizer.pad_id)\n",
        "for k, t in enumerate(ts2):\n",
        "            tokens2[k, : len(t)] = torch.tensor(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sASUjGy5TyJ",
        "outputId": "275dc9c5-7b32-47dd-f1c4-2076d951eca1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" .',\n",
              " \"The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .\",\n",
              " \"It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 .\",\n",
              " \"Troops are divided into five classes : Scouts , <unk> , Engineers , <unk> and Armored Soldier . <unk> can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types .\",\n",
              " \"Unlike its two predecessors , Valkyria Chronicles III was not released in the west . According to Sega , this was due to poor sales of Valkyria Chronicles II and the general unpopularity of the PSP in the west . An unofficial fan translation patch began development in February 2012 : players with a copy of Valkyria Chronicles III could download and apply the patch , which translated the game 's text into English . <unk> with the Extra Edition , the patch was released in January 2014 .\",\n",
              " 'On its day of release in Japan , Valkyria Chronicles III topped both platform @-@ exclusive and multi @-@ platform sales charts . By early February , the game sold 102 @,@ <unk> units , coming in second overall to The Last Story for the Wii . By the end of the year , the game had sold just over 152 @,@ 500 units .',\n",
              " 'Famitsu enjoyed the story , and were particularly pleased with the improvements to gameplay . Japanese gaming site Game Watch <unk> , despite negatively noting its pacing and elements recycled from previous games , was generally positive about its story and characters , and found its gameplay entertaining despite off @-@ putting difficulty spikes . <unk> writer <unk> <unk> , in a \" Play Test \" article based on the game \\'s <unk> demo , felt that Valkyria Chronicles III provided a \" profound feeling of closure \" for the Valkyria Chronicles series . He praised its gameplay despite annoying limitations to aspects such as special abilities , and positively noted its shift in story to a tone similar to the first game .',\n",
              " \"In a preview of the TGS demo , Ryan Geddes of IGN was left excited as to where the game would go after completing the demo , along with enjoying the improved visuals over Valkyria Chronicles II . Kotaku 's Richard <unk> was highly positive about the game , citing is story as a return to form after Valkyria Chronicles II and its gameplay being the best in the series . His main criticisms were its length and gameplay repetition , along with expressing regret that it would not be localized .\",\n",
              " 'Kurt and Riela were featured in the Nintendo 3DS crossover Project X Zone , representing the Valkyria series . Media.Vision would return to the series to develop Valkyria : Azure Revolution , with Ozawa returning as director . Azure Revolution is a role @-@ playing video game for the PlayStation 4 that forms the beginning of a new series within the Valkyria franchise .',\n",
              " \"Two manga adaptations were produced , following each of the game 's main female protagonists Imca and Riela . They were Senjō no Valkyria 3 : <unk> <unk> <unk> no <unk> ( 戦場のヴァルキュリア3 <unk> , lit . Valkyria of the Battlefield 3 : The Flower of the Nameless Oath ) , illustrated by <unk> <unk> and eventually released in two volumes after being serialized in Dengeki <unk> between 2011 and 2012 ; and Senjō no Valkyria 3 : <unk> <unk> no <unk> <unk> ( 戦場のヴァルキュリア3 <unk> , lit . Valkyria of the Battlefield 3 <unk> <unk> of the Crimson Fate ) , illustrated by <unk> <unk> and eventually released in a single volume by Kadokawa Shoten in 2012 .\"]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8HNcPRa6Erk",
        "outputId": "a237902c-169b-496d-d872-3a8256503592"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['they do to !', 'they do not !'],\n",
              " ['she okay ?', 'i hope so .'],\n",
              " ['wow', 'let s go .'],\n",
              " ['i m kidding . you know how sometimes you just become this persona ? and you don t know how to quit ?',\n",
              "  'no'],\n",
              " ['no', 'okay you re gonna need to learn how to lie .'],\n",
              " ['i figured you d get to the good stuff eventually .', 'what good stuff ?'],\n",
              " ['what good stuff ?', 'the real you .'],\n",
              " ['the real you .', 'like my fear of wearing pastels ?'],\n",
              " ['do you listen to this crap ?', 'what crap ?'],\n",
              " ['what crap ?',\n",
              "  'me . this endless . . .blonde babble . i m like boring myself .']]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGcekreA-bcA"
      },
      "source": [
        "#ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItCKW-U3-eWE"
      },
      "outputs": [],
      "source": [
        "model = Transformer(ModelArgs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN9XIGpLj6pY",
        "outputId": "3c181684-7685-4ff8-9cf0-a037137736e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (tok_embeddings): Embedding(3000, 512)\n",
              "  (layers): ModuleList(\n",
              "    (0-2): 3 x TransformerBlock(\n",
              "      (attention): Attention(\n",
              "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (wo): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (w1): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (w2): Linear(in_features=1536, out_features=512, bias=True)\n",
              "        (w3): Linear(in_features=512, out_features=1536, bias=True)\n",
              "      )\n",
              "      (attention_norm): RMSNorm()\n",
              "      (ffn_norm): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (norm): RMSNorm()\n",
              "  (output): Linear(in_features=512, out_features=3000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJCKUmXGy9Ks"
      },
      "outputs": [],
      "source": [
        "model=torch.load('model_pt.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntWidQrWFWZU"
      },
      "outputs": [],
      "source": [
        "model=torch.load('model_pre.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dOXpJfI-9ER"
      },
      "outputs": [],
      "source": [
        "model.to('cuda:0')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OVfFTmZ--4E"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4IYLChtyKK1"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a334NJH9_BIW"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"wq\",\"wk\",\"wv\",\"wo\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    #task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy4uAjIRKU22",
        "outputId": "a536c4a5-3f19-4b2c-a49c-b759d1b9ab4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model_weight_ft (3).pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLkzpadiK7iW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac3sNZ0F_DHq"
      },
      "outputs": [],
      "source": [
        "dataset = torch.utils.data.TensorDataset(tokens1[0:1280], tokens2[0:1280])\n",
        "Loader = torch.utils.data.DataLoader(dataset,\n",
        "                                         batch_size=32,\n",
        "                                         shuffle=True,\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTT1JP8Dzht4"
      },
      "outputs": [],
      "source": [
        "input=tokens1[0:10].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KuxWzUx0d4I"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4iX3pAtzliq"
      },
      "outputs": [],
      "source": [
        "model(input,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FBn0S9Y2dqi"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIzyY76Lyl32",
        "outputId": "fc28b37c-3feb-4b35-8f0a-f4bbb22daf36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model_pt_weight (1).pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAek30lM1doL"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzzV6d9v1fdr",
        "outputId": "a02c7b6c-5691-49ed-b69b-13faf19673f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 98304 || all params: 13417400 || trainable%: 0.7326605750741574\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJONY7eY_G8l"
      },
      "outputs": [],
      "source": [
        "for i, data in enumerate(Loader, 0):\n",
        "        print(i)\n",
        "        inputs, labels = data[0].to('cuda:0'),data[1].to('cuda:0')\n",
        "\n",
        "        outputs = model(inputs,0)\n",
        "        loss = criterion(outputs.transpose(1,2), labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        del inputs,labels,outputs,loss\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXZFdRCuyttG"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYphu6vGyMnE"
      },
      "outputs": [],
      "source": [
        "torch.save(model,'model_pt.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5z1WoRB_I0y"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model_weight.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbKu65zbK8i6"
      },
      "outputs": [],
      "source": [
        "ge=LM(model,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9iVFuoravDX"
      },
      "source": [
        "#rl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CygAkzvUG4u"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "191BteO-UPD-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI_1VjIZVUGH"
      },
      "outputs": [],
      "source": [
        "vocab_size=3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoYtV-0YbNCY"
      },
      "outputs": [],
      "source": [
        "class RewardModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(nn.Linear(vocab_size,vocab_size//2 , bias = False),\n",
        "                       nn.Linear(vocab_size//2,1 , bias = False))\n",
        "  def forward(self,x):\n",
        "     return self.net(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oq9GFlWT3aH"
      },
      "outputs": [],
      "source": [
        "input=tokens1[0:32].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17ciYv9wTwnk"
      },
      "outputs": [],
      "source": [
        "model=Transformer(ModelArgs)\n",
        "ge=LM(model,tokenizer)\n",
        "rewards=RewardModel()\n",
        "rewards.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WbLEeawTxbi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6fUhXGm_9Gq"
      },
      "outputs": [],
      "source": [
        "def clipped_value_loss(values, rewards, old_values, clip):\n",
        "    value_clipped = old_values + (values - old_values).clamp(-clip, clip)\n",
        "    value_loss_1 = (value_clipped.flatten() - rewards) ** 2\n",
        "    value_loss_2 = (values.flatten() - rewards) ** 2\n",
        "    return torch.mean(torch.max(value_loss_1, value_loss_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5gOxOTd_1Vi"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self,model):\n",
        "        super().__init__()\n",
        "        self.actor=model\n",
        "  def forward(self,x,mask = None,):\n",
        "      action_logits = self.actor(x,start_pos)\n",
        "\n",
        "      return action_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNlLc4HP_ykN"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self,model):\n",
        "        super().__init__()\n",
        "        self.critic=model\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(3000, 1),)\n",
        "  def forward(self,x,mask = None,start_pos=None):\n",
        "            critic_embeds = self.critic(x,start_pos)\n",
        "            out=critic_embeds[:,-1,:].clone()\n",
        "            values = self.value_head(out)\n",
        "            return values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77c8Xobi_pyN"
      },
      "outputs": [],
      "source": [
        "def Advantages(reward,values):\n",
        "  lastgaelam=0\n",
        "  advantages = torch.zeros_like(reward).to(device)\n",
        "  for t in reversed(range(len(reward))):\n",
        "    if t == len(reward) - 1:\n",
        "       nextvalues = values[t]\n",
        "    else:\n",
        "                    nextvalues = values[t + 1]\n",
        "    delta = reward[t] + gamma * nextvalues  - values[t]\n",
        "    advantages[t] = lastgaelam = delta + gamma * gae_lambda * lastgaelam\n",
        "  return advantages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXD4_jyu_rXf"
      },
      "outputs": [],
      "source": [
        "def g_token(res):\n",
        "  seq_token=[tokenizer.encode(x, bos=True, eos=True) for x in res]\n",
        "  max_batch_size: int = 32\n",
        "  max_seq_len: int = 2048\n",
        "  min_prompt_size = min([len(t) for t in seq_token])\n",
        "  max_prompt_size = max([len(t) for t in seq_token])\n",
        "  total_len = min(max_seq_len, 20 + max_prompt_size)\n",
        "\n",
        "  bsz=len(seq_token)\n",
        "\n",
        "  seq_tokens=torch.full((bsz, total_len),tokenizer.pad_id)\n",
        "  for k, t in enumerate(seq_token):\n",
        "            seq_tokens[k, : len(t)] = torch.tensor(t)\n",
        "  return seq_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv69XFm8xs6B"
      },
      "outputs": [],
      "source": [
        "model=Transformer(ModelArgs)\n",
        "model.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EIs9QiT3imS",
        "outputId": "b11b1931-ca6b-47e6-8f60-1fcf558318a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RewardModel(\n",
              "  (net): Sequential(\n",
              "    (0): Linear(in_features=3000, out_features=1500, bias=False)\n",
              "    (1): Linear(in_features=1500, out_features=1, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer(ModelArgs)\n",
        "model.to('cuda:0')\n",
        "ge=LM(model,tokenizer)\n",
        "rewards=RewardModel()\n",
        "rewards.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn1ttgX84IC5"
      },
      "outputs": [],
      "source": [
        "st = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcZ0V-K8ZzfR"
      },
      "outputs": [],
      "source": [
        "reward_loss= nn.CrossEntropyLoss()\n",
        "reward_opt=optim.Adam(rewards.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmIsNcWKUuL5"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "rewards.train()\n",
        "episodes=5\n",
        "epoch=10\n",
        "\n",
        "for _ in range(episodes):\n",
        "    sample=[randint(0, 28444) for i in range(32)]\n",
        "    input=[sentence1[i]  for i in sample]\n",
        "    label=[sentence2[i]  for i in sample]\n",
        "    res=ge.generate(input,20,0.5)\n",
        "    output=[res[i][len(input[i]):] for i in range(32)]\n",
        "    ebd1=st.encode(output, convert_to_tensor=True)\n",
        "    ebd2=st.encode(label, convert_to_tensor=True)\n",
        "    score=torch.diag(util.pytorch_cos_sim(ebd1, ebd2)).data*torch.tensor([10]).to(device)\n",
        "    seq=g_token(res)\n",
        "    a_logi=model(seq.to(device),0)\n",
        "    for j in range(epoch):\n",
        "      reward=rewards(a_logi)\n",
        "      re_loss=reward_loss(reward.squeeze(-1),score.long())\n",
        "\n",
        "      reward_opt.zero_grad()\n",
        "      re_loss.backward(retain_graph=True)\n",
        "      reward_opt.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSwh7kBNet_u"
      },
      "outputs": [],
      "source": [
        "torch.save(rewards.state_dict(), 'reward_model_weight.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcMgThCz_fLj"
      },
      "outputs": [],
      "source": [
        "eps_clip=0.2\n",
        "value_clip=0.4\n",
        "beta_s=0.01\n",
        "gamma=0.98\n",
        "gae_lambda=0.94"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LguUQqNZpuyg"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIqZHg1uhAy_"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"output\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    #task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-SS_Tokkyzq"
      },
      "outputs": [],
      "source": [
        "model=torch.load('/content/model_pt.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naMs0XrGk0Er"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3qcpim23k77"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFxNIzZf3lMR",
        "outputId": "3562259e-ef8f-4f6f-d3b8-3e7e8fbeb27e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 28096 || all params: 13445496 || trainable%: 0.20896216844659357\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jwdTMVZ_jz9"
      },
      "outputs": [],
      "source": [
        "\n",
        "actor=model\n",
        "actor.to('cuda:0')\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
        "\n",
        "critic=Critic(copy.deepcopy(actor))\n",
        "critic.to('cuda:0')\n",
        "critic_optimizer=optim.Adam(critic.parameters(), lr=0.001)\n",
        "\n",
        "rewards=RewardModel()\n",
        "rewards.to('cuda:0')\n",
        "rewards.load_state_dict(torch.load('reward_model_weight.pth'))\n",
        "ge=LLaMA(actor,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Doo_f9nBhOWg"
      },
      "outputs": [],
      "source": [
        "actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipNPoeuUUoAn"
      },
      "outputs": [],
      "source": [
        "episodes=1\n",
        "ecpoch=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk-nzwb7fn-4"
      },
      "outputs": [],
      "source": [
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP_hYey8fs3a"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqWpsP9uALym"
      },
      "outputs": [],
      "source": [
        "actor.train()\n",
        "critic.train()\n",
        "rewards.eval()\n",
        "for _ in range(episodes):\n",
        "    prompts=sample(sentence1, 32)\n",
        "    res=ge.generate(prompts,20,0.5)\n",
        "    state_action=g_token(res).to(device)\n",
        "\n",
        "    for j in range(ecpoch):\n",
        "        seq_toknes=state_action\n",
        "        action_logi=actor(seq_toknes,0)\n",
        "        reward=rewards(action_logi)\n",
        "        value=critic(seq_toknes,start_pos=0)\n",
        "        advantages=Advantages(reward,value)\n",
        "        old_action_probs=action_logi.softmax(dim=-1)[0]\n",
        "        old_log_probs=(action_logi.softmax(-1).gather(-1,seq_toknes[...,None].to(device)).squeeze(-1).log())[0]\n",
        "        old_values=value[0]\n",
        "        for i in range(len(prompts)):\n",
        "            seq=seq_toknes[i].view(1,-1)\n",
        "\n",
        "            action_logi=actor(seq,0)\n",
        "            action_prob=action_logi.softmax(dim=-1)\n",
        "            values=critic(seq,start_pos=0)\n",
        "            a_log_prob=(action_prob.gather(-1,seq[...,None].to(device)).squeeze(-1).log())\n",
        "            entropies = (action_prob* action_prob.log()).sum(dim = -1)\n",
        "            kl_divs=(action_prob*(action_prob.log()-old_action_probs.log())).sum(dim=-1)\n",
        "            reward=rewards(action_logi)\n",
        "            reward=reward-kl_divs\n",
        "            ada=reward - old_values\n",
        "            ratios = (a_log_prob - old_log_probs).exp()\n",
        "\n",
        "            surr1 = ratios * (advantages[i]+ada)\n",
        "            surr2 = ratios.clamp(1-eps_clip, 1 + eps_clip) * (advantages[i]+ada)\n",
        "            policy_loss = - torch.min(surr1, surr2) - beta_s* entropies\n",
        "            loss= policy_loss.mean()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            actor_optimizer.step()\n",
        "            actor_optimizer.zero_grad()\n",
        "\n",
        "            value_clipped=old_values+ (values - old_values).clamp(-value_clip, value_clip)\n",
        "            value_loss_1=(value_clipped.flatten() - reward) ** 2\n",
        "            value_loss_2 = (values.flatten() - reward) ** 2\n",
        "            value_loss = torch.mean(torch.max(value_loss_1, value_loss_2)).mean()\n",
        "\n",
        "            value_loss.backward(retain_graph=True)\n",
        "            critic_optimizer.step()\n",
        "            critic_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            old_action_probs=action_prob\n",
        "            old_log_probs=a_log_prob\n",
        "            old_values=values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkBtJVJLkMiu"
      },
      "outputs": [],
      "source": [
        "torch.save(actor,'model_rl_pt.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJA9GGzpiHeB"
      },
      "source": [
        "#llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u6E-H0miJvs"
      },
      "outputs": [],
      "source": [
        "model=torch.load('model_rl_ft.pth')\n",
        "model.to('cuda:0')\n",
        "ge=LM(model,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDDObLQtr0Gz"
      },
      "outputs": [],
      "source": [
        "input=sample(sentence1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nphMqgsfr2O_"
      },
      "outputs": [],
      "source": [
        "res=ge.generate(input,20,0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbvQ-M-EsCg_"
      },
      "outputs": [],
      "source": [
        "res[0][len(input[0]):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH4KsnT4iPq-"
      },
      "outputs": [],
      "source": [
        "def llm(input,pre_len,temp):\n",
        "    g_s=ge.generate(input,pre_len,temp)\n",
        "    res=g_s[len(input);]\n",
        "    return  res"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

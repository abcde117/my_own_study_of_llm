{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af2h94z50gsy"
      },
      "source": [
        "#pip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrNJCvJB0WAs"
      },
      "outputs": [],
      "source": [
        "!pip install torchdata\n",
        "!pip install 'portalocker>=2.0.0'\n",
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
        "!pip install -U sentence-transformers\n",
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJCqdmVb0iBc"
      },
      "source": [
        "#ライブラリー&helper func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exRLz3oFzJuv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from random import *\n",
        "\n",
        "import sentencepiece as spm\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "from typing import List\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR73ivdf0kwM"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MF4BkbL0oV1"
      },
      "source": [
        "#model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFCa8Wes0xKB"
      },
      "outputs": [],
      "source": [
        "class ModelArgs:\n",
        "    dim: int = 512\n",
        "    n_layers: int = 3\n",
        "    n_heads: int = 8\n",
        "    vocab_size: int = 3000 # defined later by tokenizer\n",
        "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
        "    norm_eps: float = 1e-5\n",
        "\n",
        "    max_batch_size: int = 32\n",
        "    max_seq_len: int = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyI-C5QF0oDn"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(nn.Module):\n",
        "    def __init__(self, model_path: str):\n",
        "        # reload tokenizer\n",
        "        assert os.path.isfile(model_path), model_path\n",
        "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
        "        #logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n",
        "\n",
        "        # BOS / EOS token IDs\n",
        "        self.n_words: int = self.sp_model.vocab_size()\n",
        "        self.bos_id: int = self.sp_model.bos_id()\n",
        "        self.eos_id: int = self.sp_model.eos_id()\n",
        "        self.pad_id: int = self.sp_model.pad_id()\n",
        "        #logger.info(f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\")\n",
        "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
        "\n",
        "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
        "        assert type(s) is str\n",
        "        t = self.sp_model.encode(s)\n",
        "        if bos:\n",
        "            t = [self.bos_id] + t\n",
        "        if eos:\n",
        "            t = t + [self.eos_id]\n",
        "        return t\n",
        "\n",
        "    def decode(self, t: List[int]) -> str:\n",
        "        return self.sp_model.decode(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pMJy-nP0tVY"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujjhwtDU0znE"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads=args.n_heads\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.wq=nn.Linear(args.dim,self.head_dim*args.n_heads)\n",
        "        self.wk=nn.Linear(args.dim,self.head_dim*args.n_heads)\n",
        "        self.wv=nn.Linear(args.dim,self.head_dim*args.n_heads)\n",
        "        self.wo=nn.Linear(self.head_dim*args.n_heads,args.dim)\n",
        "\n",
        "        #self.cache_k = torch.zeros(\n",
        "        #    (args.max_batch_size, args.max_seq_len, args.n_heads, self.head_dim)\n",
        "        #).cuda()\n",
        "        #self.cache_v = torch.zeros(\n",
        "           # (args.max_batch_size, args.max_seq_len, args.n_heads, self.head_dim)\n",
        "        #).cuda()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, mask: Optional[torch.Tensor]):\n",
        "        bsz, seqlen,_ = x.shape\n",
        "        xq=self.wq(x).view(bsz, -1, self.n_heads, self.head_dim)\n",
        "        xk=self.wk(x).view(bsz, -1, self.n_heads, self.head_dim)\n",
        "        xv=self.wv(x).view(bsz, -1, self.n_heads, self.head_dim)\n",
        "\n",
        "        #self.cache_k[:bsz, start_pos : start_pos + seqlen].data = xk\n",
        "        #self.cache_v[:bsz, start_pos : start_pos + seqlen].data = xv\n",
        "        #keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
        "        #values = self.cache_v[:bsz, : start_pos + seqlen]\n",
        "        xq,keys,=torch.fft.fft(xq),torch.fft.fft(xk)\n",
        "        values=xv\n",
        "        xq = xq.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        #print(xq.is_cuda)\n",
        "        #print(keys.is_cuda)\n",
        "        #print(values.is_cuda)\n",
        "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        scores=\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
        "        #scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xv)\n",
        "\n",
        "        output = torch.matmul(scores, values)\n",
        "        #output_if=torch.fft.ifft(output).real  # (bs, n_local_heads, slen, head_dim)\n",
        "        output = output.transpose(\n",
        "            1, 2\n",
        "        ).contiguous().view(bsz, seqlen, -1)\n",
        "\n",
        "        return self.wo(output).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xuk-f_E606o-"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        hidden_dim: int,\n",
        "        multiple_of: int,):\n",
        "\n",
        "        super().__init__()\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "        self.w1=nn.Linear(dim,hidden_dim)\n",
        "        self.w2=nn.Linear(hidden_dim,dim)\n",
        "        self.w3=nn.Linear(dim,hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juIvVT0308kG"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.attention = Attention(args)\n",
        "        self.feed_forward = FeedForward(dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of)\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, mask: Optional[torch.Tensor]):\n",
        "        h = x + self.attention.forward(self.attention_norm(x), start_pos, mask)\n",
        "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48kh5UdG0-eC"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, params: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.n_layers = params.n_layers\n",
        "        self.tok_embeddings=nn.Embedding( params.vocab_size, params.dim)\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for layer_id in range(params.n_layers):\n",
        "            self.layers.append(TransformerBlock(layer_id, params))\n",
        "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "        self.output=nn.Linear(params.dim,params.vocab_size)\n",
        "        #self.freqs_cis = precompute_freqs_cis(self.params.dim, self.params.max_seq_len * 2)\n",
        "\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
        "        _bsz, seqlen = tokens.shape\n",
        "        h = self.tok_embeddings(tokens)\n",
        "        #self.freqs_cis = self.freqs_cis.to(h.device)\n",
        "        #freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
        "\n",
        "        mask = None\n",
        "        if seqlen > 1:\n",
        "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
        "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, start_pos, mask)\n",
        "        h = self.norm(h)\n",
        "        output = self.output(h)\n",
        "        #u,s,vt=torch.pca_lowrank(output)\n",
        "        #output=vt.transpose(1,2)\n",
        "        return output.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCHFCYFj1A8K"
      },
      "outputs": [],
      "source": [
        "class LLaMA:\n",
        "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        max_gen_len: int,\n",
        "        temperature: float = 0.8,\n",
        "        top_p: float = 0.95,\n",
        "    ) -> List[str]:\n",
        "\n",
        "        bsz = len(prompts)\n",
        "        params = self.model.params\n",
        "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
        "\n",
        "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
        "\n",
        "        min_prompt_size = min([len(t) for t in prompt_tokens])\n",
        "        max_prompt_size = max([len(t) for t in prompt_tokens])\n",
        "\n",
        "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
        "\n",
        "        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n",
        "        for k, t in enumerate(prompt_tokens):\n",
        "            tokens[k, : len(t)] = torch.tensor(t).long()\n",
        "        input_text_mask = tokens != self.tokenizer.pad_id\n",
        "        start_pos = min_prompt_size\n",
        "        prev_pos = 0\n",
        "        for cur_pos in range(start_pos, total_len):\n",
        "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos).mean(dim=1)\n",
        "            if temperature > 0:\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                next_token = sample_top_p(probs, top_p)\n",
        "            else:\n",
        "                next_token = torch.argmax(logits, dim=-1)\n",
        "            next_token = next_token.reshape(-1)\n",
        "            # only replace token if prompt has already been generated\n",
        "            next_token = torch.where(\n",
        "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
        "            )\n",
        "            tokens[:, cur_pos] = next_token\n",
        "            prev_pos = cur_pos\n",
        "\n",
        "        decoded = []\n",
        "        for i, t in enumerate(tokens.tolist()):\n",
        "            # cut to max gen len\n",
        "            t = t[: len(prompt_tokens[i]) + max_gen_len]\n",
        "            # cut to eos tok if any\n",
        "            try:\n",
        "                t = t[: t.index(self.tokenizer.eos_id)]\n",
        "            except ValueError:\n",
        "                pass\n",
        "            decoded.append(self.tokenizer.decode(t))\n",
        "        return decoded\n",
        "\n",
        "\n",
        "def sample_top_p(probs, p):\n",
        "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    mask = probs_sum - probs_sort > p\n",
        "    probs_sort[mask] = 0.0\n",
        "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "    next_token = torch.gather(probs_idx, -1, next_token)\n",
        "    return next_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRoC8Lwx2Xk1"
      },
      "source": [
        "#token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMnSwo9N2XNY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBRgIe_320We"
      },
      "outputs": [],
      "source": [
        "spm.SentencePieceTrainer.Train(\n",
        "    input='botchan.txt',\n",
        "    model_prefix='m',\n",
        "    vocab_size=3000,\n",
        "    pad_id=3)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "tokenizer=Tokenizer('m.model')\n",
        "train_iter = WikiText2(split='train')\n",
        "sentence=[]\n",
        "for item in train_iter :\n",
        "  if item.strip() == '':pass\n",
        "\n",
        "  elif len(item.strip()) <=90:pass\n",
        "  elif len(item.strip()) >=800:pass\n",
        "  else :sentence.append((item.strip()))\n",
        "\n",
        "token=[tokenizer.encode(x, bos=True, eos=True) for x in sentence]\n",
        "max_batch_size: int = 32\n",
        "max_seq_len: int = 2048\n",
        "min_prompt_size = min([len(t) for t in token])\n",
        "max_prompt_size = max([len(t) for t in token])\n",
        "total_len = min(max_seq_len, 30 + max_prompt_size)\n",
        "\n",
        "bsz=len(token)\n",
        "\n",
        "tokens=torch.full((bsz, total_len),tokenizer.pad_id)\n",
        "for k, t in enumerate(token):\n",
        "            tokens[k, : len(t)] = torch.tensor(t)\n",
        "\n",
        "labels=torch.full((bsz, total_len),tokenizer.pad_id)\n",
        "rand = torch.rand(tokens.shape)\n",
        "mask_arr = rand < 0.15\n",
        "\n",
        "for k,t in enumerate(labels):\n",
        "     if random()>=0.1:\n",
        "            labels[k][torch.where(mask_arr[k] == 1)] =tokens[k][torch.where(mask_arr[k] == 1)]\n",
        "     else:labels[k][torch.where(mask_arr[k] == 1)]  =torch.tensor([randint(0,2999) for i in  range(len(labels[k][torch.where(mask_arr[k] == 1)])) ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el0C1qbR22CD"
      },
      "outputs": [],
      "source": [
        "lines = open('formatted_movie_lines.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "con1=[ pairs[i][0] for i in range(len(pairs))]\n",
        "con2=[ pairs[i][1] for i in range(len(pairs))]\n",
        "sentence1=[]\n",
        "sentence2=[]\n",
        "for i in range(len(con1)) :\n",
        "  if len(con1[i]) <=100 and len(con2[i])  <=100:\n",
        "    sentence1.append(con1[i])\n",
        "    sentence2.append(con2[i])\n",
        "  else : pass\n",
        "ts1=[tokenizer.encode(x, bos=True, eos=True) for x in sentence1]\n",
        "ts2=[tokenizer.encode(x, bos=True, eos=True) for x in sentence2]\n",
        "bsz=len(sentence1)\n",
        "tokens1=torch.full((bsz, 100),tokenizer.pad_id)\n",
        "for k, t in enumerate(ts1):\n",
        "            tokens1[k, : len(t)] = torch.tensor(t)\n",
        "\n",
        "tokens2=torch.full((bsz, 100),tokenizer.pad_id)\n",
        "for k, t in enumerate(ts2):\n",
        "            tokens2[k, : len(t)] = torch.tensor(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpsnqy033HTi"
      },
      "source": [
        "#pre-training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(ModelArgs)"
      ],
      "metadata": {
        "id": "ilYKrlvzXDXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model_pt_weight (1).pth'))"
      ],
      "metadata": {
        "id": "XpQYIC03dHQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cuda:0')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "7vuUmHJZZEq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgBkPE9-3ajl"
      },
      "outputs": [],
      "source": [
        "dataset = torch.utils.data.TensorDataset(tokens[0:130], labels[0:130])\n",
        "Loader = torch.utils.data.DataLoader(dataset,\n",
        "                                         batch_size=5,\n",
        "                                         shuffle=True,\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzp3GoNa3can"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "running_loss = 0.0\n",
        "\n",
        "for i, data in enumerate(Loader, 0):\n",
        "        print(i)\n",
        "        inputs, labels = data[0].to('cuda:0'),data[1].to('cuda:0')\n",
        "\n",
        "        outputs = model(inputs,0)\n",
        "        loss = criterion(outputs.transpose(1,2), labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        del inputs,labels,outputs,loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weight.pth')"
      ],
      "metadata": {
        "id": "-H0HMCQRdojo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fine-tuning"
      ],
      "metadata": {
        "id": "2CTAMe57d2jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "CIki2bXyd3w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(ModelArgs)\n"
      ],
      "metadata": {
        "id": "P6Sox0G7eDQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cuda:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSZJlhyX6b3w",
        "outputId": "f5fb566b-08b6-4cff-93f3-3cbccc9f0044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (tok_embeddings): Embedding(3000, 512)\n",
              "  (layers): ModuleList(\n",
              "    (0-2): 3 x TransformerBlock(\n",
              "      (attention): Attention(\n",
              "        (wq): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (wk): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (wv): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (wo): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (w1): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (w2): Linear(in_features=1536, out_features=512, bias=True)\n",
              "        (w3): Linear(in_features=512, out_features=1536, bias=True)\n",
              "      )\n",
              "      (attention_norm): RMSNorm()\n",
              "      (ffn_norm): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (norm): RMSNorm()\n",
              "  (output): Linear(in_features=512, out_features=3000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model_pt_weight (1).pth'))"
      ],
      "metadata": {
        "id": "gXf0uavpeIMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)"
      ],
      "metadata": {
        "id": "z40kb8h1eDBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"wq\",\"wk\",\"wv\",\"wo\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    #task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "6MMQefrHeLrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "NQFqTqLl3eOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model_weight_ft (3).pth'))"
      ],
      "metadata": {
        "id": "u4KaCkGQeOWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a9fb456-6bc9-40e2-ee87-ecd7b3c6814d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=torch.load('model_ft.pth')"
      ],
      "metadata": {
        "id": "cr9hmeEIfLds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "jcdZlu4PeVHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torch.utils.data.TensorDataset(tokens1[0:1280], tokens2[0:1280])\n",
        "Loader = torch.utils.data.DataLoader(dataset,\n",
        "                                         batch_size=32,\n",
        "                                         shuffle=True,\n",
        "                                        )"
      ],
      "metadata": {
        "id": "XG5sWcCweQYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(Loader, 0):\n",
        "        print(i)\n",
        "        inputs, labels = data[0].to('cuda:0'),data[1].to('cuda:0')\n",
        "\n",
        "        outputs = model(inputs,0)\n",
        "        loss = criterion(outputs.transpose(1,2), labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        del inputs,labels,outputs,loss\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UATQEoJUeV8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "-OwN6Trc8bH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'model_ft1.pth')"
      ],
      "metadata": {
        "id": "aH8_kKXbea7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weight_ft.pth')"
      ],
      "metadata": {
        "id": "1mWsaNQSefmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#rl"
      ],
      "metadata": {
        "id": "frsZh2O4yMvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g_token(res):\n",
        "  seq_token=[tokenizer.encode(x, bos=True, eos=True) for x in res]\n",
        "  max_batch_size: int = 32\n",
        "  max_seq_len: int = 2048\n",
        "  min_prompt_size = min([len(t) for t in seq_token])\n",
        "  max_prompt_size = max([len(t) for t in seq_token])\n",
        "  total_len = min(max_seq_len, 20 + max_prompt_size)\n",
        "\n",
        "  bsz=len(seq_token)\n",
        "\n",
        "  seq_tokens=torch.full((bsz, total_len),tokenizer.pad_id)\n",
        "  for k, t in enumerate(seq_token):\n",
        "            seq_tokens[k, : len(t)] = torch.tensor(t)\n",
        "  return seq_tokens"
      ],
      "metadata": {
        "id": "hlRqkkXZ8Dht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##reward"
      ],
      "metadata": {
        "id": "Gh87DsPH5oF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=3000"
      ],
      "metadata": {
        "id": "1-RDOO0XyMKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(nn.Linear(vocab_size,vocab_size//2 , bias = False),\n",
        "                       nn.Linear(vocab_size//2,1 , bias = False))\n",
        "  def forward(self,x):\n",
        "     return self.net(x)"
      ],
      "metadata": {
        "id": "bI1ie0ic5KXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ge=LLaMA(model,tokenizer)\n",
        "rewards=RewardModel()\n",
        "rewards.to('cuda:0')"
      ],
      "metadata": {
        "id": "6tLi4X5N5NF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "vYUWAQKb5WSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_loss= nn.CrossEntropyLoss()\n",
        "reward_opt=optim.Adam(rewards.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "CbQPfXh95ZBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "rewards.train()\n",
        "episodes=5\n",
        "epoch=10\n",
        "\n",
        "for _ in range(episodes):\n",
        "    sample=[randint(0, 28444) for i in range(32)]\n",
        "    input=[sentence1[i]  for i in sample]\n",
        "    label=[sentence2[i]  for i in sample]\n",
        "    res=ge.generate(input,20,0.5)\n",
        "    output=[res[i][len(input[i]):] for i in range(32)]\n",
        "    ebd1=st.encode(output, convert_to_tensor=True)\n",
        "    ebd2=st.encode(label, convert_to_tensor=True)\n",
        "    score=torch.diag(util.pytorch_cos_sim(ebd1, ebd2)).data*torch.tensor([10]).to(device)\n",
        "    seq=g_token(res)\n",
        "    a_logi=model(seq.to(device),0)\n",
        "    for j in range(epoch):\n",
        "      reward=rewards(a_logi)\n",
        "      re_loss=reward_loss(reward.squeeze(-1),score.long())\n",
        "\n",
        "      reward_opt.zero_grad()\n",
        "      re_loss.backward(retain_graph=True)\n",
        "      reward_opt.step()"
      ],
      "metadata": {
        "id": "v0HwY0Rd5aig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(rewards.state_dict(), 'reward_model_weight.pth')"
      ],
      "metadata": {
        "id": "A1JvoAOO5cog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=torch.load('model_ft.pth')"
      ],
      "metadata": {
        "id": "S6xtfaXX5dDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(tokens[0:32].to(device),0)"
      ],
      "metadata": {
        "id": "E65q1mUI95h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ppo&actor-critic"
      ],
      "metadata": {
        "id": "O-tL0Rcs5o7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self,model):\n",
        "        super().__init__()\n",
        "        self.critic=model\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(3000, 1),)\n",
        "  def forward(self,x,mask = None,start_pos=None):\n",
        "            critic_embeds = self.critic(x,start_pos)\n",
        "            out=critic_embeds[:,-1,:].clone()\n",
        "            values = self.value_head(out)\n",
        "            return values\n",
        "\n",
        "def Advantages(reward,values):\n",
        "  lastgaelam=0\n",
        "  advantages = torch.zeros_like(reward).to(device)\n",
        "  for t in reversed(range(len(reward))):\n",
        "    if t == len(reward) - 1:\n",
        "       nextvalues = values[t]\n",
        "    else:\n",
        "                    nextvalues = values[t + 1]\n",
        "    delta = reward[t] + gamma * nextvalues  - values[t]\n",
        "    advantages[t] = lastgaelam = delta + gamma * gae_lambda * lastgaelam\n",
        "  return advantages\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g9i0F8385tPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"output\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    #task_type=\"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "iO_xeOVs58qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps_clip=0.2\n",
        "value_clip=0.4\n",
        "beta_s=0.01\n",
        "gamma=0.98\n",
        "gae_lambda=0.94"
      ],
      "metadata": {
        "id": "xXFSGEHD5273"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)"
      ],
      "metadata": {
        "id": "CvMIkwue5-ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "mF4bIe276AgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fXyIRKG6Bsd",
        "outputId": "2e037657-a392-4252-b37c-69db3f1456e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 28096 || all params: 13445496 || trainable%: 0.20896216844659357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "Qi4rQ6639ZmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "actor=model\n",
        "actor.to('cuda:0')\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
        "\n",
        "critic=Critic(copy.deepcopy(actor))\n",
        "critic.to('cuda:0')\n",
        "critic_optimizer=optim.Adam(critic.parameters(), lr=0.001)\n",
        "\n",
        "rewards=RewardModel()\n",
        "rewards.to('cuda:0')\n",
        "rewards.load_state_dict(torch.load('reward_model_weight.pth'))\n",
        "ge=LLaMA(actor,tokenizer)"
      ],
      "metadata": {
        "id": "xwyqsVh16FE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor"
      ],
      "metadata": {
        "id": "aPutXQJg9jaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes=5\n",
        "ecpoch=5"
      ],
      "metadata": {
        "id": "P1XuRqTq-Akt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "jqCr3rqI-F6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import *"
      ],
      "metadata": {
        "id": "zQ1EuAeQ-MtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor.train()\n",
        "critic.train()\n",
        "rewards.eval()\n",
        "for _ in range(episodes):\n",
        "    prompts=sample(sentence1, 32)\n",
        "    res=ge.generate(prompts,20,0.5)\n",
        "    state_action=g_token(res).to(device)\n",
        "\n",
        "    for j in range(ecpoch):\n",
        "        seq_toknes=state_action\n",
        "        action_logi=actor(seq_toknes,0)\n",
        "        reward=rewards(action_logi)\n",
        "        value=critic(seq_toknes,start_pos=0)\n",
        "        advantages=Advantages(reward,value)\n",
        "        old_action_probs=action_logi.softmax(dim=-1)[0]\n",
        "        old_log_probs=(action_logi.softmax(-1).gather(-1,seq_toknes[...,None].to(device)).squeeze(-1).log())[0]\n",
        "        old_values=value[0]\n",
        "        for i in range(len(prompts)):\n",
        "            seq=seq_toknes[i].view(1,-1)\n",
        "\n",
        "            action_logi=actor(seq,0)\n",
        "            action_prob=action_logi.softmax(dim=-1)\n",
        "            values=critic(seq,start_pos=0)\n",
        "            a_log_prob=(action_prob.gather(-1,seq[...,None].to(device)).squeeze(-1).log())\n",
        "            entropies = (action_prob* action_prob.log()).sum(dim = -1)\n",
        "            kl_divs=(action_prob*(action_prob.log()-old_action_probs.log())).sum(dim=-1)\n",
        "            reward=rewards(action_logi)\n",
        "            reward=reward-kl_divs\n",
        "            ada=reward - old_values\n",
        "            ratios = (a_log_prob - old_log_probs).exp()\n",
        "\n",
        "            surr1 = ratios * (advantages[i]+ada)\n",
        "            surr2 = ratios.clamp(1-eps_clip, 1 + eps_clip) * (advantages[i]+ada)\n",
        "            policy_loss = - torch.min(surr1, surr2) - beta_s* entropies\n",
        "            loss= policy_loss.mean()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            actor_optimizer.step()\n",
        "            actor_optimizer.zero_grad()\n",
        "\n",
        "            value_clipped=old_values+ (values - old_values).clamp(-value_clip, value_clip)\n",
        "            value_loss_1=(value_clipped.flatten() - reward) ** 2\n",
        "            value_loss_2 = (values.flatten() - reward) ** 2\n",
        "            value_loss = torch.mean(torch.max(value_loss_1, value_loss_2)).mean()\n",
        "\n",
        "            value_loss.backward(retain_graph=True)\n",
        "            critic_optimizer.step()\n",
        "            critic_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            old_action_probs=action_prob\n",
        "            old_log_probs=a_log_prob\n",
        "            old_values=values"
      ],
      "metadata": {
        "id": "N02mv7Zr9gfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re=actor(tokens1[0:1].to(device),0)"
      ],
      "metadata": {
        "id": "7wTd4SD4-gtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(actor,'model_rl_ft.pth')"
      ],
      "metadata": {
        "id": "AkAkFPp19ovr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5GLrBL9AB6I",
        "outputId": "3917f0cf-a4f3-4706-8154-5e58735240f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100, 3000])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u,s,vt=torch.pca_lowrank(re)"
      ],
      "metadata": {
        "id": "FHk32WVJADSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax0Z8s3MBIcg",
        "outputId": "2a75f4ab-f14c-4bf0-8553-6af5c0e10699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3000, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vt.transpose(1,2)[:,0,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQRoq_bkAPMJ",
        "outputId": "c404641b-8c2f-4d53-cd93-3a2a5dcfae95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3000])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lm"
      ],
      "metadata": {
        "id": "0v93V2eF9qoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##設置"
      ],
      "metadata": {
        "id": "gtmAepjiCF55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LM:\n",
        "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        max_gen_len: int,\n",
        "        temperature: float = 0.8,\n",
        "        top_p: float = 0.95,\n",
        "    ) -> List[str]:\n",
        "\n",
        "        bsz = len(prompts)\n",
        "        params = self.model.params\n",
        "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
        "\n",
        "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
        "\n",
        "        min_prompt_size = min([len(t) for t in prompt_tokens])\n",
        "        max_prompt_size = max([len(t) for t in prompt_tokens])\n",
        "\n",
        "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
        "\n",
        "        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n",
        "        for k, t in enumerate(prompt_tokens):\n",
        "            tokens[k, : len(t)] = torch.tensor(t).long()\n",
        "        input_text_mask = tokens != self.tokenizer.pad_id\n",
        "        start_pos = min_prompt_size\n",
        "        prev_pos = 0\n",
        "        for cur_pos in range(start_pos, total_len):\n",
        "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
        "            u,s,vt=torch.pca_lowrank(logits)\n",
        "            logits=vt.transpose(1,2)[:,0,:]\n",
        "            if temperature > 0:\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                next_token = sample_top_p(probs, top_p)\n",
        "            else:\n",
        "                next_token = torch.argmax(logits, dim=-1)\n",
        "            next_token = next_token.reshape(-1)\n",
        "            # only replace token if prompt has already been generated\n",
        "            next_token = torch.where(\n",
        "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
        "            )\n",
        "            tokens[:, cur_pos] = next_token\n",
        "            prev_pos = cur_pos\n",
        "\n",
        "        decoded = []\n",
        "        for i, t in enumerate(tokens.tolist()):\n",
        "            # cut to max gen len\n",
        "            t = t[: len(prompt_tokens[i]) + max_gen_len]\n",
        "            # cut to eos tok if any\n",
        "            try:\n",
        "                t = t[: t.index(self.tokenizer.eos_id)]\n",
        "            except ValueError:\n",
        "                pass\n",
        "            decoded.append(self.tokenizer.decode(t))\n",
        "        return decoded\n",
        "\n",
        "\n",
        "def sample_top_p(probs, p):\n",
        "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    mask = probs_sum - probs_sort > p\n",
        "    probs_sort[mask] = 0.0\n",
        "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "    next_token = torch.gather(probs_idx, -1, next_token)\n",
        "    return next_token"
      ],
      "metadata": {
        "id": "4Tff2433AcTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=torch.load('model_rl_ft.pth')\n",
        "model.to('cuda:0')\n",
        "ge=LM(model,tokenizer)"
      ],
      "metadata": {
        "id": "UuyfSnTk9rYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm(input,pre_len,temp):\n",
        "    input=[input]\n",
        "    g_s=ge.generate(input,pre_len,temp)\n",
        "    res=g_s[0][len(input[0]):]\n",
        "    return  res"
      ],
      "metadata": {
        "id": "HaaDg4e79vbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=sentence1[60:61]"
      ],
      "metadata": {
        "id": "psFbWxRtA5v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeSAV-mEExqE",
        "outputId": "2c590375-41e1-45c0-8f69-9f003c6e902d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['joey never told you we went out did he ?']"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(input[0])"
      ],
      "metadata": {
        "id": "kWnKLx_UA9T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(input[0],20,0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FusZprOdA2Dr",
        "outputId": "657bb375-47d1-4c75-b62f-df3cc42c3b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' pick fine oblig held back periodating transferred possess iceump cheapavo Pthing posted[CT mi helpless'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##アプリ"
      ],
      "metadata": {
        "id": "_FOCk_9WCIDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "nw-_oKxLCun9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "yP98u1GLCAQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1[600:612]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvT0dgjjH5D8",
        "outputId": "4d9a43d8-a208-4471-ff8b-a90fca4bdc4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['if you cooperate with the da maybe they ll help you with your situation .',\n",
              " 'i will if they don t send me back .',\n",
              " 'alright . that s a justifiable homicide .',\n",
              " 'whether you tell us or not we ll find out . better if it comes from you .',\n",
              " 'if i tell you will you arrest me ?',\n",
              " 'i told your partner i can t help . i didn t see anything .',\n",
              " 'c mon start at the beginning . you know these people ?',\n",
              " 'tamina was a friend of mine . my shower was broken she let me use theirs .',\n",
              " 'oh .',\n",
              " 'oh yeah . . . ? alright .',\n",
              " 'i m ready to be briefed . excuse us .',\n",
              " 'what was i supposed to do ? the guy tried to mug me . i was gonna send a cop back i just forgot .']"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(fn=llm,\n",
        "                    inputs=[\"text\",gr.Slider(1,20),gr.Slider(0,1)],\n",
        "                    outputs=[\"text\"]\n",
        "                    )\n",
        "\n",
        "demo.launch(debug=True, inbrowser=True, share=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "8ACXeI6oA_z3",
        "outputId": "f246466c-5a88-40f0-fde0-68c44ffe3fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTtv4kmKDRzj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}